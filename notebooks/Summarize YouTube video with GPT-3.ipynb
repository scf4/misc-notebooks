{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8DxFY8Xio6j"
      },
      "source": [
        "# Summarize YouTube video with GPT-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DzPd33EBisT9"
      },
      "outputs": [],
      "source": [
        "#@title Quick start { run: \"auto\" }\n",
        "#@markdown <small>Fill out this form and then click Runtime -> Run all</small>\n",
        "\n",
        "video_url = 'https://www.youtube.com/watch?v=_q1K7cybyRk' #@param {type:\"string\"}\n",
        "\n",
        "openai_api_key = \"sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown <small>(Get an API key from http://beta.openai.com/account/api-keys)</small>\n",
        "\n",
        "whisper_model = \"large-v2\" #@param [\"large-v2\", \"medium.en\", \"small.en\"]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Wx_ASTixgMKC"
      },
      "source": [
        "# Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N-9sUb9fifsG"
      },
      "outputs": [],
      "source": [
        "#@title 1. Install libraries { display-mode: \"form\" }\n",
        "\n",
        "!pip install youtube-dl\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install openai\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "3CqtR2Fi5-vP"
      },
      "outputs": [],
      "source": [
        "# @title 2. Initial setup\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    import tensorflow\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import whisper\n",
        "import openai\n",
        "import torchaudio\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from IPython.display import Markdown\n",
        "\n",
        "import youtube_dl\n",
        "from transformers import GPT2TokenizerFast\n",
        "\n",
        "openai.api_key=openai_api_key\n",
        "\n",
        "# Initialize Whisper Model\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = whisper.load_model(whisper_model)\n",
        "\n",
        "# Disable timestamps\n",
        "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
        "\n",
        "# Tokenizer (for counting tokens)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q31DBIFWg62B"
      },
      "source": [
        "# Download audio and transcribe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "FfeZhcvjg_r7"
      },
      "outputs": [],
      "source": [
        "# @title Download audio from YouTube video\n",
        "\n",
        "mp3_file_path = \"./\" + video_url.split(\"v=\")[1].split(\"&\")[0] + \".mp3\"\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "    'outtmpl': mp3_file_path,\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '192',\n",
        "    }],\n",
        "}\n",
        "\n",
        "with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([video_url])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dYVY8MkORRIF"
      },
      "outputs": [],
      "source": [
        "# @title Transcribe Audio\n",
        "\n",
        "# Start transcription\n",
        "\n",
        "result = model.transcribe(mp3_file_path)\n",
        "\n",
        "transcript = result['text'].strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf-nMIgDhjCs"
      },
      "source": [
        "# Generate summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "d0PCRrxmnviU",
        "outputId": "1033cc2e-7ded-4444-8671-28f6a307a6e8"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Next.js 13.1: What's New?\n",
              "\n",
              "## Create Next App\n",
              "\n",
              "- The Create Next App has been redesigned and revamped in 13.1, with a fancy new design and the use of newer Next.js features like Next font optimization.\n",
              "- To get started with Create Next App, run `npx create-next-app` in your terminal.\n",
              "\n",
              "## App Directory Improvements\n",
              "\n",
              "- The app directory in 13.1 includes improvements to both the routing and the data fetching system.\n",
              "- The wrapping element around nested layouts has been removed.\n",
              "- The amount of client side JavaScript needed inside of the app directory has been reduced by 20 kilobytes.\n",
              "- The new `modularizeImports` option allows for remapping of imports from named exports to full paths.\n",
              "\n",
              "## Edge Runtime for API Routes\n",
              "\n",
              "- The now stable edge runtime is a lighter node.js runtime built on web standard APIs.\n",
              "- It allows for a strict subset of all of the available node.js APIs that align with the web platform and edge computing platforms.\n",
              "- Edge functions on Vercel are now generally available.\n",
              "\n",
              "## Middleware Improvements\n",
              "\n",
              "- Middleware allows for full flexibility over routing in Next.js.\n",
              "- It can be used to check if a user has access to a specific resource, modify the request headers for use in get server side props or in an API route, and produce a response.\n",
              "\n",
              "## Turbo Pack Support\n",
              "\n",
              "- Support for using Next.js with Turbo Pack, an incremental bundler built with Rust, has been added in 13.1.\n",
              "- This includes Tailwind support, Post CSS support, Next image support, and stability and reliability improvements.\n",
              "- To try it out, use the `--turbo` flag in your package.json."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title Summary\n",
        "\n",
        "prompt = f'''\n",
        "The text below is transcribed from audio.\n",
        "\n",
        "Instructions:\n",
        "\n",
        "- Write a title that faithfully sums up the speaker's talk in the format \"# <title>\"\n",
        "\n",
        "- Break the talk into sections with subheadings — make sure to format them as markdown!\n",
        "\n",
        "- Rewrite each section, focusing on its main points without losing the impact intended by the speaker. Consider using bullet points for readability.\n",
        "\n",
        "- Stay faithful to the speaker's intent/perspective.\n",
        "\n",
        "- When listing things (e.g., a list of examples that the speaker gives) you should use bullet points to make it easier to read.\n",
        "\n",
        "\"\"\"\n",
        "{transcript}\n",
        "\"\"\"\n",
        "'''\n",
        "\n",
        "max_tokens = 4000 - len(tokenizer(prompt)['input_ids'])\n",
        "\n",
        "summary = openai.Completion.create(\n",
        "  model=\"text-davinci-003\",\n",
        "  temperature=0.2,\n",
        "  prompt=prompt,\n",
        "  max_tokens=max_tokens\n",
        ")['choices'][0]['text'].strip()\n",
        "\n",
        "display(Markdown(summary))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Wx_ASTixgMKC",
        "q31DBIFWg62B"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
